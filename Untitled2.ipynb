{
 "metadata": {
  "name": "",
  "signature": "sha256:07d48df6401383206526fd11fe79da4481893f8cd03efd32fdd4d1c4b28a3927"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "MDP"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"Markov Decision Processes (Chapter 17)\n",
      "\n",
      "First we define an MDP, and the special case of a GridMDP, in which\n",
      "states are laid out in a 2-dimensional grid.  We also represent a policy\n",
      "as a dictionary of {state:action} pairs, and a Utility function as a\n",
      "dictionary of {state:number} pairs.  We then define the value_iteration\n",
      "and policy_iteration algorithms.\"\"\"\n",
      "\n",
      "\n",
      "class MDP:\n",
      "    \"\"\"A Markov Decision Process, defined by an initial state, transition model,\n",
      "    and reward function. We also keep track of a gamma value, for use by\n",
      "    algorithms. The transition model is represented somewhat differently from\n",
      "    the text.  Instead of P(s' | s, a) being a probability number for each\n",
      "    state/state/action triplet, we instead have T(s, a) return a list of (p, s')\n",
      "    pairs.  We also keep track of the possible states, terminal states, and\n",
      "    actions for each state. [page 646]\"\"\"\n",
      "\n",
      "    def __init__(self, init, actlist, terminals, gamma=.9):\n",
      "        update(self, init=init, actlist=actlist, terminals=terminals,\n",
      "               gamma=gamma, states=set(), reward={})\n",
      "\n",
      "    def R(self, state):\n",
      "        \"Return a numeric reward for this state.\"\n",
      "        return self.reward[state]\n",
      "\n",
      "    def T(self, state, action):\n",
      "        \"\"\"Transition model.  From a state and an action, return a list\n",
      "        of (probability, result-state) pairs.\"\"\"\n",
      "        abstract\n",
      "\n",
      "    def actions(self, state):\n",
      "        \"\"\"Set of actions that can be performed in this state.  By default, a\n",
      "        fixed list of actions, except for terminal states. Override this\n",
      "        method if you need to specialize by state.\"\"\"\n",
      "        if state in self.terminals:\n",
      "            return [None]\n",
      "        else:\n",
      "            return self.actlist\n",
      "\n",
      "class GridMDP(MDP):\n",
      "    \"\"\"A two-dimensional grid MDP, as in [Figure 17.1].  All you have to do is\n",
      "    specify the grid as a list of lists of rewards; use None for an obstacle\n",
      "    (unreachable state).  Also, you should specify the terminal states.\n",
      "    An action is an (x, y) unit vector; e.g. (1, 0) means move east.\"\"\"\n",
      "    def __init__(self, grid, terminals, init=(0, 0), gamma=.9):\n",
      "        grid.reverse() ## because we want row 0 on bottom, not on top\n",
      "        MDP.__init__(self, init, actlist=orientations,\n",
      "                     terminals=terminals, gamma=gamma)\n",
      "        update(self, grid=grid, rows=len(grid), cols=len(grid[0]))\n",
      "        for x in range(self.cols):\n",
      "            for y in range(self.rows):\n",
      "                self.reward[x, y] = grid[y][x]\n",
      "                if grid[y][x] is not None:\n",
      "                    self.states.add((x, y))\n",
      "\n",
      "    def T(self, state, action):\n",
      "        if action is None:\n",
      "            return [(0.0, state)]\n",
      "        else:\n",
      "            return [(0.8, self.go(state, action)),\n",
      "                    (0.1, self.go(state, turn_right(action))),\n",
      "                    (0.1, self.go(state, turn_left(action)))]\n",
      "\n",
      "    def go(self, state, direction):\n",
      "        \"Return the state that results from going in this direction.\"\n",
      "        state1 = vector_add(state, direction)\n",
      "        return if_(state1 in self.states, state1, state)\n",
      "\n",
      "    def to_grid(self, mapping):\n",
      "        \"\"\"Convert a mapping from (x, y) to v into a [[..., v, ...]] grid.\"\"\"\n",
      "        return list(reversed([[mapping.get((x,y), None)\n",
      "                               for x in range(self.cols)]\n",
      "                              for y in range(self.rows)]))\n",
      "\n",
      "    def to_arrows(self, policy):\n",
      "        chars = {(1, 0):'>', (0, 1):'^', (-1, 0):'<', (0, -1):'v', None: '.'}\n",
      "        return self.to_grid(dict([(s, chars[a]) for (s, a) in policy.items()]))\n",
      "\n",
      "#______________________________________________________________________________\n",
      "\n",
      "Fig[17,1] = GridMDP([[-0.04, -0.04, -0.04, +1],\n",
      "                     [-0.04, None,  -0.04, -1],\n",
      "                     [-0.04, -0.04, -0.04, -0.04]],\n",
      "                    terminals=[(3, 2), (3, 1)])\n",
      "\n",
      "#______________________________________________________________________________\n",
      "\n",
      "def value_iteration(mdp, epsilon=0.001):\n",
      "    \"Solving an MDP by value iteration. [Fig. 17.4]\"\n",
      "    U1 = dict([(s, 0) for s in mdp.states])\n",
      "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
      "    while True:\n",
      "        U = U1.copy()\n",
      "        delta = 0\n",
      "        for s in mdp.states:\n",
      "            U1[s] = R(s) + gamma * max([sum([p * U[s1] for (p, s1) in T(s, a)])\n",
      "                                        for a in mdp.actions(s)])\n",
      "            delta = max(delta, abs(U1[s] - U[s]))\n",
      "        if delta < epsilon * (1 - gamma) / gamma:\n",
      "             return U\n",
      "\n",
      "def best_policy(mdp, U):\n",
      "    \"\"\"Given an MDP and a utility function U, determine the best policy,\n",
      "    as a mapping from state to action. (Equation 17.4)\"\"\"\n",
      "    pi = {}\n",
      "    for s in mdp.states:\n",
      "        pi[s] = argmax(mdp.actions(s), lambda a:expected_utility(a, s, U, mdp))\n",
      "    return pi\n",
      "\n",
      "def expected_utility(a, s, U, mdp):\n",
      "    \"The expected utility of doing a in state s, according to the MDP and U.\"\n",
      "    return sum([p * U[s1] for (p, s1) in mdp.T(s, a)])\n",
      "\n",
      "#______________________________________________________________________________\n",
      "\n",
      "def policy_iteration(mdp):\n",
      "    \"Solve an MDP by policy iteration [Fig. 17.7]\"\n",
      "    U = dict([(s, 0) for s in mdp.states])\n",
      "    pi = dict([(s, random.choice(mdp.actions(s))) for s in mdp.states])\n",
      "    while True:\n",
      "        U = policy_evaluation(pi, U, mdp)\n",
      "        unchanged = True\n",
      "        for s in mdp.states:\n",
      "            a = argmax(mdp.actions(s), lambda a: expected_utility(a,s,U,mdp))\n",
      "            if a != pi[s]:\n",
      "                pi[s] = a\n",
      "                unchanged = False\n",
      "        if unchanged:\n",
      "            return pi\n",
      "\n",
      "def policy_evaluation(pi, U, mdp, k=20):\n",
      "    \"\"\"Return an updated utility mapping U from each state in the MDP to its\n",
      "    utility, using an approximation (modified policy iteration).\"\"\"\n",
      "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
      "    for i in range(k):\n",
      "        for s in mdp.states:\n",
      "            U[s] = R(s) + gamma * sum([p * U[s1] for (p, s1) in T(s, pi[s])])\n",
      "    return U\n",
      "\n",
      "__doc__ += \"\"\"\n",
      ">>> pi = best_policy(Fig[17,1], value_iteration(Fig[17,1], .01))\n",
      "\n",
      ">>> Fig[17,1].to_arrows(pi)\n",
      "[['>', '>', '>', '.'], ['^', None, '^', '.'], ['^', '>', '^', '<']]\n",
      "\n",
      ">>> print_table(Fig[17,1].to_arrows(pi))\n",
      ">   >      >   .\n",
      "^   None   ^   .\n",
      "^   >      ^   <\n",
      "\n",
      ">>> print_table(Fig[17,1].to_arrows(policy_iteration(Fig[17,1])))\n",
      ">   >      >   .\n",
      "^   None   ^   .\n",
      "^   >      ^   <\n",
      "\"\"\"\n",
      "\n",
      "__doc__ += random_tests(\"\"\"\n",
      ">>> pi\n",
      "{(3, 2): None, (3, 1): None, (3, 0): (-1, 0), (2, 1): (0, 1), (0, 2): (1, 0), (1, 0): (1, 0), (0, 0): (0, 1), (1, 2): (1, 0), (2, 0): (0, 1), (0, 1): (0, 1), (2, 2): (1, 0)}\n",
      "\n",
      ">>> value_iteration(Fig[17,1], .01)\n",
      "{(3, 2): 1.0, (3, 1): -1.0, (3, 0): 0.12958868267972745, (0, 1): 0.39810203830605462, (0, 2): 0.50928545646220924, (1, 0): 0.25348746162470537, (0, 0): 0.29543540628363629, (1, 2): 0.64958064617168676, (2, 0): 0.34461306281476806, (2, 1): 0.48643676237737926, (2, 2): 0.79536093684710951}\n",
      "\n",
      ">>> policy_iteration(Fig[17,1])\n",
      "{(3, 2): None, (3, 1): None, (3, 0): (0, -1), (2, 1): (-1, 0), (0, 2): (1, 0), (1, 0): (1, 0), (0, 0): (1, 0), (1, 2): (1, 0), (2, 0): (1, 0), (0, 1): (1, 0), (2, 2): (1, 0)}\n",
      "\n",
      "\"\"\")\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "global name 'orientations' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-3-06a287ecc042>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     83\u001b[0m                      \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.04\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;33m-\u001b[0m\u001b[1;36m0.04\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                      [-0.04, -0.04, -0.04, -0.04]],\n\u001b[1;32m---> 85\u001b[1;33m                     terminals=[(3, 2), (3, 1)])\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;31m#______________________________________________________________________________\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-3-06a287ecc042>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, grid, terminals, init, gamma)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m## because we want row 0 on bottom, not on top\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         MDP.__init__(self, init, actlist=orientations,\n\u001b[0m\u001b[0;32m     49\u001b[0m                      terminals=terminals, gamma=gamma)\n\u001b[0;32m     50\u001b[0m         \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: global name 'orientations' is not defined"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "0.2*log(0.0000001) + 0.8*log(0.9999999)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "-3.2236192101916683"
       ]
      }
     ],
     "prompt_number": 8
    }
   ],
   "metadata": {}
  }
 ]
}